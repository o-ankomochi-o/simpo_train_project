#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
OpenAIè©•ä¾¡ã«åŸºã¥ãæå¤±ã‚’çµ„ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
"""

import json
import os
import time
from typing import Dict, List, Literal, Union

import torch
import torch.nn.functional as F
import wandb
from openai import OpenAI
from trl import CPOTrainer


class GenerationEvaluationTrainer(CPOTrainer):
    """
    æ–‡ç« ç”Ÿæˆãƒ»è©•ä¾¡ãƒ»æå¤±çµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼

    OpenAIã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã—ã€ãã®çµæœã‚’æå¤±é–¢æ•°ã«çµ„ã¿è¾¼ã¿ã¾ã™ã€‚
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        training_args = kwargs["args"]

        # ç‹¬è‡ªæå¤±é–¢é€£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.simpo_gamma = getattr(training_args, "simpo_gamma", 0.8)
        self.loss_type = getattr(training_args, "loss_type", "sigmoid")

        # è©•ä¾¡é‡ã¿ä»˜ã‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.eval_loss_weight = getattr(training_args, "eval_loss_weight", 0.1)

        # ç”Ÿæˆé–¢é€£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.enable_generation = getattr(training_args, "enable_generation", True)
        self.generation_interval = getattr(training_args, "generation_interval", 25)
        self.generation_batch_size = getattr(training_args, "generation_batch_size", 2)

        # OpenAIè©•ä¾¡é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.enable_openai_eval = getattr(training_args, "openai_evaluation", False)
        self.openai_model = getattr(training_args, "openai_model", "gpt-4o-2024-08-06")
        self.openai_api_key = getattr(
            training_args, "openai_api_key", os.environ.get("OPENAI_API_KEY", "")
        )

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.enable_openai_eval and self.openai_api_key:
            try:
                self.openai_client = OpenAI(api_key=self.openai_api_key)
                print(f"OpenAI client initialized with model: {self.openai_model}")
            except Exception as e:
                print(f"Failed to initialize OpenAI client: {str(e)}")
                self.enable_openai_eval = False
        else:
            self.enable_openai_eval = False
            if self.enable_openai_eval:
                print("OpenAI evaluation disabled: API key not provided")

        self.step_counter = 0

        # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ã™ã‚‹å¤‰æ•°
        self.latest_eval_scores = {}
        self.steps_since_last_eval = 0

        # ç”Ÿæˆã¨è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.generation_dir = os.path.join(training_args.output_dir, "generations")
        os.makedirs(self.generation_dir, exist_ok=True)

        print(f"Initialized GenerationEvaluationTrainer with parameters:")
        print(f"  - simpo_gamma: {self.simpo_gamma}")
        print(f"  - loss_type: {self.loss_type}")
        print(f"  - eval_loss_weight: {self.eval_loss_weight}")
        print(f"  - generation_interval: {self.generation_interval}")

    def log(self, logs, start_time=None):
        """
        æ‹¡å¼µãƒ­ã‚°æ©Ÿèƒ½ - super().log() ã‚’å‘¼ã°ãšã«ç‹¬è‡ªç®¡ç†
        """
        print(f"ğŸ“ ãƒ­ã‚°è¨˜éŒ²ç™ºç”Ÿï¼ç¾åœ¨ã® global_step: {self.state.global_step}")
        print(f"logsã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼: {list(logs.keys())}")
        print(f"è©•ä¾¡é–¢é€£ã®ã‚­ãƒ¼: {[k for k in logs.keys() if k.startswith('eval_')]}")

        # æ•°å€¤ãƒ­ã‚°ã ã‘ãƒ•ã‚£ãƒ«ã‚¿
        numeric_logs = {k: v for k, v in logs.items() if isinstance(v, (int, float))}

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ“Šã€ã‚¹ãƒ†ãƒƒãƒ— {self.state.global_step}ã€‘WandBã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§:")

        print("è©•ä¾¡é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ä¸­:")
        for k, v in numeric_logs.items():
            if k.startswith("eval_"):
                print(f"ã€€ğŸ”¹ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {k}: {v}")
            else:
                print(f"ã€€ğŸ”¸ {k}: {v}")

        # wandb ãƒ­ã‚°
        if self.args.report_to == "wandb":
            wandb.log(numeric_logs, step=self.state.global_step)

    def evaluate_with_openai(self, prompt, generated_text):
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã™ã‚‹"""
        if not self.enable_openai_eval or not hasattr(self, "openai_client"):
            return {"error": "OpenAI evaluation not enabled"}

        try:
            # è©•ä¾¡æŒ‡ç¤º
            evaluation_prompt = f"""
            è©•ä¾¡ã—ã¦ãã ã•ã„:

            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:
            {prompt}

            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:
            {generated_text}

            ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦0ã€œ5ã®æ•°å€¤ã§è©•ä¾¡ã—ã€ãã‚Œãã‚Œã®ç†ç”±ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ï¼š
            1. é–¢é€£æ€§: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹
            2. å¤šæ§˜æ€§: è¡¨ç¾ã‚„èªå½™ã®å¤šæ§˜æ€§ãŒã‚ã‚‹ã‹
            3. è¨´æ±‚ç‚¹: ç•°ãªã‚‹è¦³ç‚¹ã‚„ä¸»å¼µã‚’å«ã‚“ã§ã„ã‚‹ã‹
            4. èª­ã¿ã‚„ã™ã•: æ–‡ç« æ§‹é€ ã‚„æµã‚Œã®è‡ªç„¶ã•
            5. å…¨ä½“è©•ä¾¡: ç·åˆçš„ãªè³ª

            çµæœã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:
            ```json
            {{
            "relevance": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "diversity": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "appeals": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "readability": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "overall": {{"score": æ•°å€¤, "reason": "ç†ç”±"}}
            }}
            """

            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯æ–‡ç« è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚æŒ‡ç¤ºã«å¾“ã£ã¦ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚",
                    },
                    {"role": "user", "content": evaluation_prompt},
                ],
                temperature=0,
            )

            response_text = response.choices[0].message.content.strip()
            response_text = response_text.strip("```json").strip("```").strip()
            print("json")
            print(response_text)

            # JSONã®æŠ½å‡º
            try:
                # JSONéƒ¨åˆ†ã®æŠ½å‡º
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response_text[json_start:json_end]
                    evaluation_result = json.loads(json_str)
                else:
                    # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ä½“ã‚’JSONã¨ã—ã¦è§£æ
                    evaluation_result = json.loads(response_text)

                return evaluation_result
            except json.JSONDecodeError:
                print(f"Failed to parse OpenAI response as JSON: {response_text}")
                return {
                    "error": "Failed to parse response",
                    "raw_response": response_text,
                }

        except Exception as e:
            print(f"Error in OpenAI evaluation: {str(e)}")
            return {"error": str(e)}

    def extract_metrics_from_evaluation(self, evaluation_result):
        """OpenAIè©•ä¾¡çµæœã‹ã‚‰ã‚¹ã‚³ã‚¢ã¨ç†ç”±ã‚’æŠ½å‡ºã—ã€ç¢ºå®Ÿã«æ•°å€¤å‹ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦è¨˜éŒ²"""
        metrics = {}

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "error" in evaluation_result:
            print(f"è©•ä¾¡ã®ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã—ãŸ: {evaluation_result['error']}")
            return {"evaluation_error": 1.0}

        try:
            score_keys = ["relevance", "diversity", "appeals", "readability", "overall"]
            score_sum = 0.0  # ç¢ºå®Ÿã« float å‹ã§åˆæœŸåŒ–
            count = 0

            for key in score_keys:
                if key in evaluation_result and isinstance(
                    evaluation_result[key], dict
                ):
                    # score å€¤ã‚’å–å¾—
                    score = evaluation_result[key].get("score", None)
                    reason = evaluation_result[key].get("reason", "")
                    print(f"ğŸ” ã‚­ãƒ¼: {key}")
                    print(f"  - ã‚¹ã‚³ã‚¢: {score}")
                    print(f"  - ç†ç”±: {reason}")

                    # ã‚¹ã‚³ã‚¢ã‚’ç¢ºå®Ÿã«æ•°å€¤å‹ã«å¤‰æ›
                    if score is not None:
                        try:
                            # int, float, str ãªã©æ§˜ã€…ãªå‹ã«å¯¾å¿œ
                            float_score = float(score)
                            # å¿…ãš float å‹ã¨ã—ã¦æ ¼ç´
                            metrics[f"eval_{key}"] = float_score
                            metrics[f"eval_{key}_reason"] = reason
                            score_sum += float_score
                            count += 1
                        except (ValueError, TypeError):
                            print(f"âš ï¸ ã‚¹ã‚³ã‚¢ã®æ•°å€¤å¤‰æ›ã«å¤±æ•—: {key} â†’ {score}")
                    else:
                        print(
                            f"âš ï¸ ã‚¹ã‚³ã‚¢ãŒä¸æ­£ã¾ãŸã¯æ¬ è½: {key} â†’ {evaluation_result[key]}"
                        )
                else:
                    print(f"âš ï¸ ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„ã‹å½¢å¼ãŒä¸æ­£: {key}")

            # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            if count > 0:
                metrics["eval_average_score"] = float(score_sum / count)
                print(f"âœ… å¹³å‡ã‚¹ã‚³ã‚¢: {metrics['eval_average_score']}")

            # æ•°å€¤å‹ç¢ºèªã®ãŸã‚ã®è¿½åŠ ãƒ­ã‚°
            for k, v in metrics.items():
                if k.startswith("eval_") and not k.endswith("_reason"):
                    print(f"ğŸ”¢ ç¢ºèª: {k} ã¯ {type(v).__name__} å‹ã§å€¤ã¯ {v}")

            return metrics

        except Exception as e:
            print(f"Error extracting metrics: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¿…è¦æœ€å°é™ã®æƒ…å ±ã®ã¿
            return {"evaluation_parsing_error": 1.0}

    def compute_evaluation_loss(self):
        """è©•ä¾¡ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æå¤±ã‚’è¨ˆç®—"""
        # è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã¯ã‚¼ãƒ­æå¤±ã‚’è¿”ã™
        if (
            not self.latest_eval_scores
            or "eval_average_score" not in self.latest_eval_scores
        ):
            return torch.tensor(0.0, device=self.model.device)

        # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‹ã‚‰æå¤±ã‚’è¨ˆç®—
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æå¤±ãŒä½ããªã‚‹ã‚ˆã†ã«è² ã®ç¬¦å·ã‚’ã¤ã‘ã‚‹
        # ã‚¹ã‚³ã‚¢ã¯é€šå¸¸0-5ãªã®ã§ã€é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹
        eval_score = self.latest_eval_scores.get("eval_average_score", 0)

        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æå¤±ãŒå°ã•ããªã‚‹ã‚ˆã†ã«å¤‰æ›
        # 5ç‚¹æº€ç‚¹ãªã‚‰ã€5-score ã§0ã«è¿‘ã¥ãã»ã©è‰¯ã„ã“ã¨ã«ãªã‚‹
        eval_loss = (5.0 - eval_score) / 5.0

        scaled_loss = eval_loss

        return torch.tensor(scaled_loss, device=self.model.device)

    def generate_samples(self, model, batch):
        """ãƒãƒƒãƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã™ã‚‹é–¢æ•°"""
        # å°ã•ã„ãƒãƒƒãƒã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡åŒ–
        mini_batch = {
            "prompt_input_ids": batch["prompt_input_ids"][: self.generation_batch_size],
            "prompt_attention_mask": batch["prompt_attention_mask"][
                : self.generation_batch_size
            ],
        }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ã‚Œã°ï¼‰ã‚’å–å¾—
        prompt_texts = []
        if "prompt" in batch:
            prompt_texts = batch["prompt"][: self.generation_batch_size]

        # ç”Ÿæˆå‰ã«å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        was_gradient_checkpointing_enabled = False
        if (
            hasattr(model, "is_gradient_checkpointing")
            and model.is_gradient_checkpointing
        ):
            was_gradient_checkpointing_enabled = True
            model.gradient_checkpointing_disable()

        try:
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=mini_batch["prompt_input_ids"],
                    attention_mask=mini_batch["prompt_attention_mask"],
                    max_new_tokens=128,
                    do_sample=True,
                    temperature=0.8,
                    num_beams=1,
                    pad_token_id=self.processing_class.pad_token_id,
                )

                decoded_texts = self.processing_class.batch_decode(
                    outputs, skip_special_tokens=True
                )
        finally:
            # å…ƒã®è¨­å®šã«æˆ»ã™
            if was_gradient_checkpointing_enabled:
                model.gradient_checkpointing_enable()

        # è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        evaluations = []
        evaluation_metrics = {}

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç”Ÿæˆæ–‡ã‚’è¡¨ç¤º
        print("\n===== ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ« =====")
        for i, text in enumerate(decoded_texts):
            if i < len(prompt_texts):
                prompt = prompt_texts[i]
                response = text[len(prompt) :]
                print(f"[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i}]: {prompt}")
                print(f"[ç”Ÿæˆå¿œç­” {i}]: {response}")
                print("-" * 80)

                # OpenAIã«ã‚ˆã‚‹è©•ä¾¡ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                if self.enable_openai_eval:
                    print(f"OpenAIè©•ä¾¡ä¸­... ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i}")
                    evaluation = self.evaluate_with_openai(prompt, response)
                    print("==============openaiã®è©•ä¾¡===============")
                    print(evaluation)
                    print("==========================================")
                    evaluations.append(
                        {
                            "prompt": prompt,
                            "response": response,
                            "evaluation": evaluation,
                        }
                    )

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦é›†è¨ˆ
                    sample_metrics = self.extract_metrics_from_evaluation(evaluation)
                    for key, value in sample_metrics.items():
                        if key in evaluation_metrics:
                            evaluation_metrics[key].append(value)
                        else:
                            evaluation_metrics[key] = [value]

                    # è©•ä¾¡çµæœã‚’è¡¨ç¤º
                    print(
                        f"è©•ä¾¡çµæœ: {json.dumps(evaluation, ensure_ascii=False, indent=2)}"
                    )
                    print("-" * 80)
            else:
                print(f"[ç”Ÿæˆå…¨æ–‡ {i}]: {text}")
                print("-" * 80)

        # è©•ä¾¡çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        if self.enable_openai_eval and evaluations:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            eval_file = os.path.join(
                self.generation_dir, f"eval_step_{self.step_counter}_{timestamp}.json"
            )
            with open(eval_file, "w", encoding="utf-8") as f:
                json.dump(evaluations, f, ensure_ascii=False, indent=2)
            print(f"è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {eval_file}")

        # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        avg_metrics = {}
        for key, values in evaluation_metrics.items():
            if isinstance(values[0], (int, float)):  # æ•°å€¤ã ã‘å¹³å‡ã™ã‚‹
                avg_metrics[key] = sum(values) / len(values)
            else:
                # æ–‡å­—åˆ—ï¼ˆä¾‹ï¼šç†ç”±ï¼‰ãªã©ã¯æœ€åˆã®ä¸€ã¤ã ã‘ä»£è¡¨ã§æ®‹ã™
                avg_metrics[key] = values[0]

        # è©•ä¾¡çµæœã‚’ä¿å­˜
        self.latest_eval_scores = avg_metrics
        self.steps_since_last_eval = 0

        return decoded_texts, avg_metrics

    def get_batch_loss_metrics(
        self,
        model,
        batch,
        train_eval: Literal["train", "eval"] = "train",
    ):
        evaluation_metrics = {}

        # ä¸€å®šé–“éš”ã§ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆã¨è©•ä¾¡
        if (
            train_eval == "train"
            and self.enable_generation
            and self.step_counter % self.generation_interval == 0
        ):
            print(f"\n[Step {self.step_counter}] ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆå®Ÿè¡Œ")
            generated_texts, evaluation_metrics = self.generate_samples(model, batch)
            print(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(generated_texts)}")

        # è¦ªã‚¯ãƒ©ã‚¹(CPOTrainer)ã®æ–¹æ³•ã‚’å‘¼ã³å‡ºã™
        loss, metrics = super().get_batch_loss_metrics(model, batch, train_eval)

        if train_eval == "train":
            # è©•ä¾¡ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æå¤±ã‚’è¨ˆç®—
            eval_loss = self.compute_evaluation_loss()

            # æå¤±ã«åŠ ç®—
            loss = loss + self.eval_loss_weight * eval_loss

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
            metrics["evaluation_based_loss"] = eval_loss.item()

            # è©•ä¾¡ã‹ã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ›´æ–°
            self.steps_since_last_eval += 1

        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆåŒæœŸå‡¦ç†ã‚’é©ç”¨ï¼‰
        print(f"è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å†…å®¹: {evaluation_metrics}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
        for key, value in evaluation_metrics.items():
            if not key.endswith("_reason"):  # ç†ç”±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯é™¤å¤–
                # ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦ã‹ã‚‰åŒæœŸå‡¦ç†
                if isinstance(value, (int, float)):
                    tensor_value = torch.tensor(value, device=self.model.device)
                    # acceleratorã‚’ä½¿ã£ã¦åŒæœŸ
                    metrics[key] = (
                        self.accelerator.gather_for_metrics(tensor_value).mean().item()
                    )
            else:
                # ç†ç”±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãã®ã¾ã¾ï¼ˆãŸã ã—ãƒ­ã‚°ã«ã¯å«ã¾ã‚Œãªã„å¯èƒ½æ€§ã‚ã‚Šï¼‰
                metrics[key] = value

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°
        if train_eval == "train":
            self.step_counter += 1

        print(f"æœ€çµ‚çš„ãªmetricsè¾æ›¸ã®å†…å®¹: {metrics}")

        return loss, metrics
