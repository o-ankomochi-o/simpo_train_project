#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
OpenAIè©•ä¾¡ã«åŸºã¥ãæå¤±ã‚’çµ„ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
"""

import json
import os
import time
from typing import Dict, List, Literal, Union

import torch
import torch.nn.functional as F
import wandb
from openai import OpenAI
from trl import CPOTrainer


class GenerationEvaluationTrainer(CPOTrainer):
    """
    æ–‡ç« ç”Ÿæˆãƒ»è©•ä¾¡ãƒ»æå¤±çµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼

    OpenAIã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã—ã€ãã®çµæœã‚’æå¤±é–¢æ•°ã«çµ„ã¿è¾¼ã¿ã¾ã™ã€‚
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        training_args = kwargs["args"]

        # ç‹¬è‡ªæå¤±é–¢é€£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.simpo_gamma = getattr(training_args, "simpo_gamma", 0.8)
        self.loss_type = getattr(training_args, "loss_type", "sigmoid")

        # è©•ä¾¡é‡ã¿ä»˜ã‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.eval_loss_weight = getattr(training_args, "eval_loss_weight", 0.1)

        # ç”Ÿæˆé–¢é€£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.enable_generation = getattr(training_args, "enable_generation", True)
        self.generation_interval = getattr(training_args, "generation_interval", 25)
        self.generation_batch_size = getattr(training_args, "generation_batch_size", 2)

        # OpenAIè©•ä¾¡é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.enable_openai_eval = getattr(training_args, "openai_evaluation", False)
        self.openai_model = getattr(training_args, "openai_model", "gpt-4o-2024-08-06")
        self.openai_api_key = getattr(
            training_args, "openai_api_key", os.environ.get("OPENAI_API_KEY", "")
        )

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.enable_openai_eval and self.openai_api_key:
            try:
                self.openai_client = OpenAI(api_key=self.openai_api_key)
                print(f"OpenAI client initialized with model: {self.openai_model}")
            except Exception as e:
                print(f"Failed to initialize OpenAI client: {str(e)}")
                self.enable_openai_eval = False
        else:
            self.enable_openai_eval = False
            if self.enable_openai_eval:
                print("OpenAI evaluation disabled: API key not provided")

        self.step_counter = 0

        # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ã™ã‚‹å¤‰æ•°
        self.latest_eval_scores = {}
        self.steps_since_last_eval = 0

        # ç”Ÿæˆã¨è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.generation_dir = os.path.join(training_args.output_dir, "generations")
        os.makedirs(self.generation_dir, exist_ok=True)

        print(f"Initialized GenerationEvaluationTrainer with parameters:")
        print(f"  - simpo_gamma: {self.simpo_gamma}")
        print(f"  - loss_type: {self.loss_type}")
        print(f"  - eval_loss_weight: {self.eval_loss_weight}")
        print(f"  - generation_interval: {self.generation_interval}")

    # def log(self, logs, start_time=None):
    #     """
    #     æ‹¡å¼µãƒ­ã‚°æ©Ÿèƒ½ - super().log() ã‚’å‘¼ã°ãšã«ç‹¬è‡ªç®¡ç†
    #     """
    #     print(f"ğŸ“ ãƒ­ã‚°è¨˜éŒ²ç™ºç”Ÿï¼ç¾åœ¨ã® global_step: {self.state.global_step}")
    #     print(f"logsã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼: {list(logs.keys())}")
    #     print(f"è©•ä¾¡é–¢é€£ã®ã‚­ãƒ¼: {[k for k in logs.keys() if k.startswith('eval_')]}")

    #     # æ•°å€¤ãƒ­ã‚°ã ã‘ãƒ•ã‚£ãƒ«ã‚¿
    #     numeric_logs = {k: v for k, v in logs.items() if isinstance(v, (int, float))}

    #     # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ­ã‚°å‡ºåŠ›
    #     print(f"ğŸ“Šã€ã‚¹ãƒ†ãƒƒãƒ— {self.state.global_step}ã€‘WandBã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§:")

    #     print("è©•ä¾¡é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ä¸­:")
    #     for k, v in numeric_logs.items():
    #         if k.startswith("eval_"):
    #             print(f"ã€€ğŸ”¹ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {k}: {v}")
    #         else:
    #             print(f"ã€€ğŸ”¸ {k}: {v}")

    #     # wandb ãƒ­ã‚°
    #     if self.args.report_to == "wandb":
    #         wandb.log(numeric_logs, step=self.state.global_step)
    def log(self, logs, start_time=None):
        """
        æ‹¡å¼µãƒ­ã‚°æ©Ÿèƒ½ - wandbã‚µãƒãƒ¼ãƒˆä»˜ã+ æ–‡å­—åˆ—ç³»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–
        """
        # æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã ã‘ã‚’æ¸¡ã™
        numeric_logs = {k: v for k, v in logs.items() if isinstance(v, (int, float))}
        # è¦ªã‚¯ãƒ©ã‚¹ã®logãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        super().log(numeric_logs)

        # è¦ªã‚¯ãƒ©ã‚¹ã®å‡¦ç†å¾Œã«wandbã«ã‚‚è¨˜éŒ²
        if self.args.report_to == "wandb":
            wandb.log(numeric_logs)

    def evaluate_with_openai(self, prompt, generated_text):
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã™ã‚‹"""
        if not self.enable_openai_eval or not hasattr(self, "openai_client"):
            return {"error": "OpenAI evaluation not enabled"}

        try:
            # è©•ä¾¡æŒ‡ç¤º
            evaluation_prompt = f"""
            è©•ä¾¡ã—ã¦ãã ã•ã„:

            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:
            {prompt}

            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:
            {generated_text}

            ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦0ã€œ5ã®æ•°å€¤ã§è©•ä¾¡ã—ã€ãã‚Œãã‚Œã®ç†ç”±ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ï¼š
            1. é–¢é€£æ€§: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹
            2. å¤šæ§˜æ€§: è¡¨ç¾ã‚„èªå½™ã®å¤šæ§˜æ€§ãŒã‚ã‚‹ã‹
            3. è¨´æ±‚ç‚¹: ç•°ãªã‚‹è¦³ç‚¹ã‚„ä¸»å¼µã‚’å«ã‚“ã§ã„ã‚‹ã‹
            4. èª­ã¿ã‚„ã™ã•: æ–‡ç« æ§‹é€ ã‚„æµã‚Œã®è‡ªç„¶ã•
            5. å…¨ä½“è©•ä¾¡: ç·åˆçš„ãªè³ª

            çµæœã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:
            ```json
            {{
            "relevance": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "diversity": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "appeals": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "readability": {{"score": æ•°å€¤, "reason": "ç†ç”±"}},
            "overall": {{"score": æ•°å€¤, "reason": "ç†ç”±"}}
            }}
            """

            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯æ–‡ç« è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚æŒ‡ç¤ºã«å¾“ã£ã¦ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚",
                    },
                    {"role": "user", "content": evaluation_prompt},
                ],
                temperature=0,
            )

            response_text = response.choices[0].message.content.strip()
            response_text = response_text.strip("```json").strip("```").strip()
            print("json")
            print(response_text)

            # JSONã®æŠ½å‡º
            try:
                # JSONéƒ¨åˆ†ã®æŠ½å‡º
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response_text[json_start:json_end]
                    evaluation_result = json.loads(json_str)
                else:
                    # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ä½“ã‚’JSONã¨ã—ã¦è§£æ
                    evaluation_result = json.loads(response_text)

                return evaluation_result
            except json.JSONDecodeError:
                print(f"Failed to parse OpenAI response as JSON: {response_text}")
                return {
                    "error": "Failed to parse response",
                    "raw_response": response_text,
                }

        except Exception as e:
            print(f"Error in OpenAI evaluation: {str(e)}")
            return {"error": str(e)}

    def extract_metrics_from_evaluation(self, evaluation_result):
        """OpenAIè©•ä¾¡çµæœã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºã—ã€ç¢ºå®Ÿã«æ•°å€¤å‹ã§è¿”ã™"""
        metrics = {}

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "error" in evaluation_result:
            return {"evaluation_error": 1.0}

        try:
            score_keys = ["relevance", "diversity", "appeals", "readability", "overall"]
            score_sum = 0.0
            count = 0

            for key in score_keys:
                if key in evaluation_result and isinstance(
                    evaluation_result[key], dict
                ):
                    # ã‚¹ã‚³ã‚¢å–å¾—ã¨æ•°å€¤å¤‰æ›ã‚’ä¸€åº¦ã«
                    try:
                        score = float(evaluation_result[key].get("score", 0))
                        metrics[f"eval_{key}"] = score
                        metrics[f"eval_{key}_reason"] = evaluation_result[key].get(
                            "reason", ""
                        )
                        score_sum += score
                        count += 1
                    except (ValueError, TypeError):
                        pass  # å¤‰æ›ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

            # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
            if count > 0:
                metrics["eval_average_score"] = score_sum / count

            return metrics

        except Exception as e:
            return {"evaluation_parsing_error": 1.0}

    def compute_evaluation_loss(self):
        """è©•ä¾¡ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æå¤±ã‚’è¨ˆç®—"""
        # è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã¯ã‚¼ãƒ­æå¤±ã‚’è¿”ã™
        if (
            not self.latest_eval_scores
            or "eval_average_score" not in self.latest_eval_scores
        ):
            return torch.tensor(0.0, device=self.model.device)

        # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‹ã‚‰æå¤±ã‚’è¨ˆç®—
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æå¤±ãŒä½ããªã‚‹ã‚ˆã†ã«è² ã®ç¬¦å·ã‚’ã¤ã‘ã‚‹
        # ã‚¹ã‚³ã‚¢ã¯é€šå¸¸0-5ãªã®ã§ã€é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹
        eval_score = self.latest_eval_scores.get("eval_average_score", 0)

        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æå¤±ãŒå°ã•ããªã‚‹ã‚ˆã†ã«å¤‰æ›
        # 5ç‚¹æº€ç‚¹ãªã‚‰ã€5-score ã§0ã«è¿‘ã¥ãã»ã©è‰¯ã„ã“ã¨ã«ãªã‚‹
        eval_loss = (5.0 - eval_score) / 5.0

        scaled_loss = eval_loss

        return torch.tensor(scaled_loss, device=self.model.device)

    def generate_samples(self, model, batch):
        """ãƒãƒƒãƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã™ã‚‹é–¢æ•°"""
        # å°ã•ã„ãƒãƒƒãƒã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡åŒ–
        mini_batch = {
            "prompt_input_ids": batch["prompt_input_ids"][: self.generation_batch_size],
            "prompt_attention_mask": batch["prompt_attention_mask"][
                : self.generation_batch_size
            ],
        }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ã‚Œã°ï¼‰ã‚’å–å¾—
        prompt_texts = []
        if "prompt" in batch:
            prompt_texts = batch["prompt"][: self.generation_batch_size]

        # ç”Ÿæˆå‰ã«å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        was_gradient_checkpointing_enabled = False
        if (
            hasattr(model, "is_gradient_checkpointing")
            and model.is_gradient_checkpointing
        ):
            was_gradient_checkpointing_enabled = True
            model.gradient_checkpointing_disable()

        try:
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=mini_batch["prompt_input_ids"],
                    attention_mask=mini_batch["prompt_attention_mask"],
                    max_new_tokens=128,
                    do_sample=True,
                    temperature=0.8,
                    num_beams=1,
                    pad_token_id=self.processing_class.pad_token_id,
                )

                decoded_texts = self.processing_class.batch_decode(
                    outputs, skip_special_tokens=True
                )
        finally:
            # å…ƒã®è¨­å®šã«æˆ»ã™
            if was_gradient_checkpointing_enabled:
                model.gradient_checkpointing_enable()

        # è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        evaluations = []
        evaluation_metrics = {}

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç”Ÿæˆæ–‡ã‚’è¡¨ç¤º
        print("\n===== ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ« =====")
        for i, text in enumerate(decoded_texts):
            if i < len(prompt_texts):
                prompt = prompt_texts[i]
                response = text[len(prompt) :]
                print(f"[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i}]: {prompt}")
                print(f"[ç”Ÿæˆå¿œç­” {i}]: {response}")
                print("-" * 80)

                # OpenAIã«ã‚ˆã‚‹è©•ä¾¡ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                if self.enable_openai_eval:
                    print(f"OpenAIè©•ä¾¡ä¸­... ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i}")
                    evaluation = self.evaluate_with_openai(prompt, response)
                    print("==============openaiã®è©•ä¾¡===============")
                    print(evaluation)
                    print("==========================================")
                    evaluations.append(
                        {
                            "prompt": prompt,
                            "response": response,
                            "evaluation": evaluation,
                        }
                    )

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦é›†è¨ˆ
                    sample_metrics = self.extract_metrics_from_evaluation(evaluation)
                    for key, value in sample_metrics.items():
                        if key in evaluation_metrics:
                            evaluation_metrics[key].append(value)
                        else:
                            evaluation_metrics[key] = [value]

                    # è©•ä¾¡çµæœã‚’è¡¨ç¤º
                    print(
                        f"è©•ä¾¡çµæœ: {json.dumps(evaluation, ensure_ascii=False, indent=2)}"
                    )
                    print("-" * 80)
            else:
                print(f"[ç”Ÿæˆå…¨æ–‡ {i}]: {text}")
                print("-" * 80)

        # è©•ä¾¡çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        if self.enable_openai_eval and evaluations:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            eval_file = os.path.join(
                self.generation_dir, f"eval_step_{self.step_counter}_{timestamp}.json"
            )
            with open(eval_file, "w", encoding="utf-8") as f:
                json.dump(evaluations, f, ensure_ascii=False, indent=2)
            print(f"è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {eval_file}")

        # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        avg_metrics = {}
        for key, values in evaluation_metrics.items():
            if isinstance(values[0], (int, float)):  # æ•°å€¤ã ã‘å¹³å‡ã™ã‚‹
                avg_metrics[key] = sum(values) / len(values)
            else:
                # æ–‡å­—åˆ—ï¼ˆä¾‹ï¼šç†ç”±ï¼‰ãªã©ã¯æœ€åˆã®ä¸€ã¤ã ã‘ä»£è¡¨ã§æ®‹ã™
                avg_metrics[key] = values[0]

        # è©•ä¾¡çµæœã‚’ä¿å­˜
        self.latest_eval_scores = avg_metrics
        self.steps_since_last_eval = 0

        return decoded_texts, avg_metrics

    def get_batch_loss_metrics(
        self,
        model,
        batch,
        train_eval: Literal["train", "eval"] = "train",
    ):
        evaluation_metrics = {}

        # ä¸€å®šé–“éš”ã§ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆã¨è©•ä¾¡
        if (
            train_eval == "train"
            and self.enable_generation
            and self.step_counter % self.generation_interval == 0
        ):
            print(f"\n[Step {self.step_counter}] ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆå®Ÿè¡Œ")
            generated_texts, evaluation_metrics = self.generate_samples(model, batch)
            print(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(generated_texts)}")

        # è¦ªã‚¯ãƒ©ã‚¹(CPOTrainer)ã®æ–¹æ³•ã‚’å‘¼ã³å‡ºã™
        loss, metrics = super().get_batch_loss_metrics(model, batch, train_eval)

        if train_eval == "train":
            # è©•ä¾¡ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æå¤±ã‚’è¨ˆç®—
            eval_loss = self.compute_evaluation_loss()

            # æå¤±ã«åŠ ç®—
            loss = loss + self.eval_loss_weight * eval_loss

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
            metrics["evaluation_based_loss"] = eval_loss.item()

            # è©•ä¾¡ã‹ã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ›´æ–°
            self.steps_since_last_eval += 1

        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆåŒæœŸå‡¦ç†ã‚’é©ç”¨ï¼‰
        print(f"è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å†…å®¹: {evaluation_metrics}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
        if evaluation_metrics:
            # æ–°ã—ã„åå‰ã®è¦å‰‡ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            prefix = "eval_" if train_eval == "eval" else ""

            # ã‚­ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã®å®šç¾©
            key_mapping = {
                "eval_relevance": f"{prefix}openai/relevance",
                "eval_diversity": f"{prefix}openai/diversity",
                "eval_appeals": f"{prefix}openai/appeals",
                "eval_readability": f"{prefix}openai/readability",
                "eval_overall": f"{prefix}openai/overall",
                "eval_average_score": f"{prefix}openai/average_score",
            }

            # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å‡¦ç†ï¼ˆç†ç”±ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»¥å¤–ï¼‰
            for key, value in evaluation_metrics.items():
                if not key.endswith("_reason") and isinstance(value, (int, float)):
                    # æ–°ã—ã„ã‚­ãƒ¼åã«å¤‰æ›
                    new_key = key_mapping.get(key, key)

                    # ãƒ†ãƒ³ã‚½ãƒ«åŒ–ã—ã¦åŒæœŸå‡¦ç†
                    tensor_value = torch.tensor(value, device=self.model.device)
                    metrics[new_key] = (
                        self.accelerator.gather_for_metrics(tensor_value).mean().item()
                    )

            # è©•ä¾¡ãƒ™ãƒ¼ã‚¹ã®æå¤±ã‚‚æ–°ã—ã„å‘½åè¦å‰‡ã§è¿½åŠ 
            if train_eval == "train" and "evaluation_based_loss" in metrics:
                metrics[f"{prefix}openai/loss"] = metrics["evaluation_based_loss"]

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°
        if train_eval == "train":
            self.step_counter += 1

        print(f"æœ€çµ‚çš„ãªmetricsè¾æ›¸ã®å†…å®¹: {metrics}")

        return loss, metrics
